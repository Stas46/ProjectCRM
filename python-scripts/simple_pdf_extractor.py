#!/usr/bin/env python3
"""
Simple PDF Text Extractor
–ü—Ä–æ—Å—Ç–æ–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
"""

import sys
import os
import json
import base64
import re
from pathlib import Path
import argparse

def extract_text_simple(pdf_path):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF –ø—É—Ç–µ–º –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    try:
        with open(pdf_path, 'rb') as file:
            pdf_content = file.read()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø–æ–∏—Å–∫–∞
        pdf_text = pdf_content.decode('latin1', errors='ignore')
        
        print(f"DEBUG: PDF —Ä–∞–∑–º–µ—Ä: {len(pdf_content)} –±–∞–π—Ç")
        
        # –ò—â–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ PDF
        text_objects = []
        raw_matches = []
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞
        patterns = [
            (r'BT\s+(.*?)\s+ET', 'BT_ET_blocks'),  # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ –º–µ–∂–¥—É BT –∏ ET
            (r'\((.*?)\)\s*Tj', 'simple_Tj'),      # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç —Å Tj
            (r'\[(.*?)\]\s*TJ', 'array_TJ'),       # –ú–∞—Å—Å–∏–≤—ã —Ç–µ–∫—Å—Ç–∞ —Å TJ
            (r'\((.*?)\)\s*TJ', 'simple_TJ'),      # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç —Å TJ
            (r'>\s*BDC\s+(.*?)\s+EMC', 'BDC_EMC'), # –ë–ª–æ–∫–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            (r'/F\d+\s+\d+\s+Tf\s+(.*?)(?=\s*/F|\s*BT|\s*ET|$)', 'font_text'), # –¢–µ–∫—Å—Ç –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —à—Ä–∏—Ñ—Ç–∞
        ]
        
        for pattern, pattern_name in patterns:
            matches = re.findall(pattern, pdf_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                raw_matches.append((pattern_name, match[:100]))  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
                
                # –û—á–∏—â–∞–µ–º –æ—Ç PDF –∫–æ–º–∞–Ω–¥ –∏ —Å–∏–º–≤–æ–ª–æ–≤
                clean_text = match
                
                # –£–¥–∞–ª—è–µ–º PDF –∫–æ–º–∞–Ω–¥—ã
                clean_text = re.sub(r'\b(?:Td|TD|Tm|TL|Tc|Tw|Tz|TZ|Ts|Tr|gs|G|g|RG|rg|K|k|CS|cs|SC|sc|SCN|scn|sh|Do|BI|ID|EI|q|Q|cm|w|J|j|M|d|ri|i|n|h|v|y|c|s|S|f|F|B)\b', '', clean_text)
                
                # –£–¥–∞–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã
                clean_text = re.sub(r'\b\d+(?:\.\d+)?\s+(?:\d+(?:\.\d+)?\s+)*(?:Td|TD|Tm|TL|Tc|Tw|Tz|TZ|Ts|Tr|gs|G|g|RG|rg|K|k|CS|cs|SC|sc|SCN|scn|sh|Do|BI|ID|EI|q|Q|cm|w|J|j|M|d|ri|i|n|h|v|y|c|s|S|f|F|B)', '', clean_text)
                
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
                clean_text = re.sub(r'[^\w\s\.,:;!?\-–∞-—è—ë–ê-–Ø–Å0-9]', ' ', clean_text)
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                words = clean_text.split()
                valid_words = []
                
                for word in words:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ —Å –±—É–∫–≤–∞–º–∏ (—Ä—É—Å—Å–∫–∏–º–∏ –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º–∏) –∏–ª–∏ —á–∏—Å–ª–∞
                    if (len(word) > 1 and 
                        (re.search(r'[–∞-—è—ë–ê-–Ø–Åa-zA-Z]', word) or word.isdigit())):
                        valid_words.append(word)
                
                if valid_words:
                    text_objects.extend(valid_words)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ (—Ç–µ–∫—Å—Ç)
        parentheses_pattern = r'\(([^)]+)\)'
        parentheses_matches = re.findall(parentheses_pattern, pdf_text)
        
        for match in parentheses_matches:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            decoded = match.replace('\\\\', '\\').replace('\\(', '(').replace('\\)', ')')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏–ª–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if re.search(r'[–∞-—è—ë–ê-–Ø–Å]', decoded) or (len(decoded) > 3 and re.search(r'[a-zA-Z0-9]', decoded)):
                words = decoded.split()
                for word in words:
                    if len(word) > 1:
                        text_objects.append(word)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_objects = []
        for obj in text_objects:
            if obj not in seen and len(obj.strip()) > 1:
                seen.add(obj)
                unique_objects.append(obj.strip())
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
        full_text = ' '.join(unique_objects)
        
        print(f"DEBUG: –ù–∞–π–¥–µ–Ω–æ {len(raw_matches)} —Å—ã—Ä—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        print(f"DEBUG: –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(unique_objects)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤")
        print(f"DEBUG: –ü—Ä–∏–º–µ—Ä—ã: {unique_objects[:5]}")
        
        return {
            "success": True,
            "full_text": full_text,
            "text_objects": unique_objects[:50],  # –ü–µ—Ä–≤—ã–µ 50 –æ–±—ä–µ–∫—Ç–æ–≤
            "objects_count": len(unique_objects),
            "raw_matches_count": len(raw_matches),
            "debug_info": f"Patterns matched: {len(raw_matches)}, Unique objects: {len(unique_objects)}"
        }
        
    except Exception as e:
        print(f"ERROR: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "full_text": "",
            "text_objects": [],
            "objects_count": 0
        }

def analyze_text_simple(text):
    """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return {}
    
    # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    words = text.split()
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    
    # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    russian_words = re.findall(r'[–∞-—è—ë]+', text, re.IGNORECASE)
    numbers = re.findall(r'\d+', text)
    dates = re.findall(r'\d{1,2}[./-]\d{1,2}[./-]\d{2,4}', text)
    amounts = re.findall(r'\d+[\s,.]?\d*\s*(—Ä—É–±|‚ÇΩ|RUB|—Ä—É–±–ª–µ–π)', text, re.IGNORECASE)
    
    return {
        "total_chars": len(text),
        "total_words": len(words),
        "total_lines": len(lines),
        "russian_words": len(russian_words),
        "numbers_found": len(numbers),
        "dates_found": len(dates),
        "amounts_found": len(amounts),
        "sample_russian_words": russian_words[:10],
        "sample_dates": dates[:5],
        "sample_amounts": amounts[:5],
        "sample_text": text[:500] + "..." if len(text) > 500 else text
    }

def create_text_report(pdf_filename, text_extraction, text_analysis):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç"""
    report = []
    report.append(f"=== PDF –ê–ù–ê–õ–ò–ó: {pdf_filename} ===")
    report.append("")
    
    if text_extraction.get("success"):
        report.append("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        if text_analysis:
            report.append(f"   ‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {text_analysis.get('total_chars', 0)}")
            report.append(f"   ‚Ä¢ –°–ª–æ–≤: {text_analysis.get('total_words', 0)}")
            report.append(f"   ‚Ä¢ –°—Ç—Ä–æ–∫: {text_analysis.get('total_lines', 0)}")
            report.append(f"   ‚Ä¢ –†—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤: {text_analysis.get('russian_words', 0)}")
            report.append(f"   ‚Ä¢ –ß–∏—Å–µ–ª: {text_analysis.get('numbers_found', 0)}")
            report.append(f"   ‚Ä¢ –î–∞—Ç: {text_analysis.get('dates_found', 0)}")
            report.append(f"   ‚Ä¢ –°—É–º–º: {text_analysis.get('amounts_found', 0)}")
        
        report.append("")
        
        # –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if text_analysis and text_analysis.get('sample_dates'):
            report.append("üìÖ –ù–ê–ô–î–ï–ù–ù–´–ï –î–ê–¢–´:")
            for date in text_analysis['sample_dates']:
                report.append(f"   ‚Ä¢ {date}")
            report.append("")
        
        if text_analysis and text_analysis.get('sample_amounts'):
            report.append("üí∞ –ù–ê–ô–î–ï–ù–ù–´–ï –°–£–ú–ú–´:")
            for amount in text_analysis['sample_amounts']:
                report.append(f"   ‚Ä¢ {amount}")
            report.append("")
        
        if text_analysis and text_analysis.get('sample_russian_words'):
            report.append("üìù –ü–†–ò–ú–ï–†–´ –°–õ–û–í:")
            words_line = ", ".join(text_analysis['sample_russian_words'])
            report.append(f"   {words_line}")
            report.append("")
        
        # –û–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞
        if text_analysis and text_analysis.get('sample_text'):
            report.append("üìÑ –û–ë–†–ê–ó–ï–¶ –¢–ï–ö–°–¢–ê:")
            report.append("   " + text_analysis['sample_text'].replace('\n', '\n   '))
            report.append("")
        
        # –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã
        if text_extraction.get('text_objects'):
            report.append("üîç –ù–ê–ô–î–ï–ù–ù–´–ï –¢–ï–ö–°–¢–û–í–´–ï –û–ë–™–ï–ö–¢–´:")
            for i, obj in enumerate(text_extraction['text_objects'], 1):
                report.append(f"   {i}. {obj}")
    
    else:
        report.append("‚ùå –û–®–ò–ë–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê:")
        report.append(f"   {text_extraction.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
    
    return "\n".join(report)

def main():
    parser = argparse.ArgumentParser(description='Simple PDF Text Extractor')
    parser.add_argument('pdf_path', help='Path to PDF file')
    parser.add_argument('--extract-text', action='store_true', help='Extract text from PDF')
    parser.add_argument('--create-report', action='store_true', help='Create text report')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.pdf_path):
        print(json.dumps({"success": False, "error": "PDF file not found"}, ensure_ascii=False))
        return
    
    result = {
        "success": True,
        "pdf_path": args.pdf_path,
        "filename": os.path.basename(args.pdf_path),
        "converter": "Simple Python Extractor"
    }
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    if args.extract_text:
        text_result = extract_text_simple(args.pdf_path)
        result["text_extraction"] = text_result
        
        if text_result.get("success") and text_result.get("full_text"):
            text_analysis = analyze_text_simple(text_result["full_text"])
            result["text_analysis"] = text_analysis
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
            if args.create_report:
                report = create_text_report(
                    result["filename"],
                    text_result,
                    text_analysis
                )
                result["text_report"] = report
    
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()